{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "references\n",
    "https://arxiv.org/pdf/1803.09588.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Models Comparisons\n",
    "\n",
    "in this notebook we will use different kinds of Image classification models and compare them on the given categories. \n",
    "The first one will be looking at is accuracy and Loss because they're our prioties during this training and also the time it takes to predict the image. \n",
    "\n",
    "1. Setup\n",
    "2. Data preparation (data spliting and labeling)\n",
    "3. Data cleaning and Exploration\n",
    "4. Building Model\n",
    "    - ImageNet\n",
    "    - VGG-16\n",
    "    - High Resolution CNN\n",
    "    - CoAtNet-7\n",
    "    - ViT-G/14\n",
    "5. Ensemble Approaches\n",
    "6. Comparisons\n",
    "7. Results\n",
    "\n",
    "\n",
    "## 1. Setup\n",
    "This Python 3 environment comes with many helpful analytics libraries installed  \n",
    "It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python  \n",
    "For example, here's several helpful packages to load.   \n",
    "In this stage we will use this data set https://paperswithcode.com/dataset/tiny-imagenet. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-13 15:39:21.841251: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-13 15:39:21.841332: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import warnings  \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline \n",
    "import tensorflow as tf \n",
    "\n",
    "import cv2 \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "from tqdm import tqdm\n",
    "import random \n",
    "import pickle \n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import train_test_split  \n",
    "import os \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Preparation\n",
    "\n",
    "\n",
    "## Data set Used: \n",
    "\n",
    "- folder descriptions: \n",
    "    * **dataset** - where datasets stored. Not yet splitted or specified to current project.\n",
    "    * **load-datasets**- where python files where stored that loads our datasets \n",
    "    * **data** - after the excecution load-datasets . All of the data will be stored in the dota folder\n",
    "\n",
    "**Image Classification Datasets**  \n",
    "This datasets will determine the best image classification that will be used in our project.  \n",
    "all of this dataset will be trained on the same model with the same hyper paramters. \n",
    "\n",
    "0. **`Asoul Image`** -  \n",
    "Dataset about v-tubers around the world. Testing for cartoon classifications\n",
    "\n",
    "1. **`Food-11 dataset`** -  \n",
    "This is a dataset containing 16643 food images grouped in 11 major food categories.  \n",
    "The 11 categories are Bread, Dairy product, Dessert, Egg, Fried food,  \n",
    "Meat, Noodles/Pasta, Rice, Seafood, Soup, and Vegetable/Fruit. \n",
    "\n",
    "\n",
    "2. `Recursion Cellular Image Classification` –   \n",
    "This data comes from the Recursion 2019 challenge.  \n",
    "This goal of the competition was to use biological  \n",
    "microscopy data to develop a model that identifies replicates.  \n",
    "The full information regarding the competition can be found here. \n",
    "\n",
    "3. `TensorFlow patch_camelyon Medical Images` –  \n",
    "This medical image classification dataset comes from the TensorFlow website.  \n",
    "It contains just over 327,000 color images, each 96 x 96 pixels.   \n",
    "The images are histopathological lymph node scans which contain metastatic tissue.\n",
    "\n",
    "4. `CoastSat Image Classification Dataset` –  \n",
    "Used for an open-source shoreline mapping tool,  \n",
    "this dataset includes aerial images taken from satellites.  \n",
    "The dataset also includes meta data pertaining to the labels. \n",
    "\n",
    "5. `Indoor Scenes Images` –  \n",
    "From MIT, this dataset contains over 15,000 images of indoor locations.  \n",
    "The dataset was originally built to tackle the problem of indoor scene recognition.  \n",
    "All images are in JPEG format and have been divided into 67 categories.  \n",
    "The number of images per category vary. However, there are at least 100 images for each category. \n",
    "\n",
    "6. `Intel Image Classification` – Created by Intel for an image classification contest,  \n",
    "this expansive image dataset contains approximately 25,000 images.   \n",
    "Furthermore, the images are divided into the following categories: buildings, forest, glacier, mountain, sea, and street. \n",
    "\n",
    "7. `TensorFlow Sun397 Image Classification Dataset` – \n",
    "Another dataset from Tensorflow,  \n",
    "this dataset contains over 108,000 images used in the Scene Understanding (SUN) benchmark.  \n",
    "Furthermore, the images have been divided into 397 categories.  \n",
    "The exact amount of images in each category varies.  \n",
    "However, there are at least 100 images in each of the various scene and object categories. \n",
    "\n",
    "### Data Loaders\n",
    "**Info:** Download the data first before preceeding to the next cell or else it will not work .  \n",
    "You can run `data_loaders.ipynb` to download and extract the data. \n",
    "\n",
    "**Warning**\n",
    "Running the cell below will download all of the datasets given above and  \n",
    "store it for our train test split. Make sure that you have 20gb of data left in your storage.\n",
    "\n",
    "**Arguments**\n",
    "> !python [`<file_name>`].py [`--download/-D`]  [`--save/-S`]\n",
    "\n",
    "* `filename` - Name of python file \n",
    "* `--download/-D` - Downloads the file. If zipped. It will automatically unzipped . Path directory will be sett on dataset\n",
    "* `--save/-S` - saving the dataset that has been extracted and converted to `X_train`, `X_test`, `y_train`, `y_test`\n",
    "\n",
    "> !python load-datasets/asoul-image-load.py  -D -S  \n",
    "> !python load-datasets/cellular-image.py --download --save  \n",
    "> !python load-datasets/food-11.py --download --save  \n",
    "> !python load-datasets/indoor-scenes.py --download --save  \n",
    "> !python load-datasets/intel-image.py --download --save  \n",
    "> !python load-datasets/medical-image.py --download --save  \n",
    "> !python load-datasets/sum397.py --download --save  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "74a71eff7ab66ee71630148c34f65e695a7aa29c70bd77083c7a3b7e6031e80d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('Deeplearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
